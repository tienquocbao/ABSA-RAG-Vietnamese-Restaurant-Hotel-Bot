{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12270834,"sourceType":"datasetVersion","datasetId":7732679},{"sourceId":12324746,"sourceType":"datasetVersion","datasetId":7768742},{"sourceId":13645451,"sourceType":"datasetVersion","datasetId":8674316}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a768a8e3-0ae0-4e06-b0c9-3122233e5106","cell_type":"markdown","source":"# ğŸš€ API PhÃ¢n tÃ­ch Cáº£m xÃºc (HTML + LLM + Flask) ğŸš€\n\nNotebook nÃ y sáº½ khá»Ÿi táº¡o má»™t **á»©ng dá»¥ng web** hoÃ n chá»‰nh:\n1.  **Frontend HTML:** Má»™t trang web Ä‘á»ƒ nháº­p review.\n2.  **Backend Flask:** Xá»­ lÃ½ yÃªu cáº§u.\n3.  **ABSA Model:** Cháº¡y model ViT5-large cá»§a báº¡n Ä‘á»ƒ phÃ¢n tÃ­ch.\n4.  **LLM Summarizer:** DÃ¹ng má»™t model ViT5-base Ä‘á»ƒ tÃ³m táº¯t káº¿t quáº£ thÃ nh ngÃ´n ngá»¯ tá»± nhiÃªn.\n5.  **Ngrok:** ÄÆ°a trang web ra Internet.\n\n## ğŸ›‘ VIá»†C Cáº¦N LÃ€M (Ráº¥t quan trá»ng!)\n\nTrÆ°á»›c khi cháº¡y, báº¡n **Báº®T BUá»˜C** pháº£i lÃ m 3 viá»‡c sau:\n\n### 1. Upload Model Checkpoints (.ckpt)\n\n1.  Táº¡o má»™t **Kaggle Dataset** Má»šI.\n2.  Upload 2 file model Ä‘Ã£ huáº¥n luyá»‡n cá»§a báº¡n (vÃ­ dá»¥: `restaurant_best_model.ckpt` vÃ  `hotel_best_model.ckpt`) lÃªn dataset Ä‘Ã³.\n3.  ThÃªm dataset nÃ y vÃ o notebook Kaggle báº±ng cÃ¡ch nháº¥n vÃ o **\"Add Data\"** (á»Ÿ thanh bÃªn pháº£i).\n\n### 2. Láº¥y Ngrok Auth Token\n\n1.  ÄÄƒng nháº­p vÃ o [Ngrok Dashboard](https://dashboard.ngrok.com/login).\n2.  á» má»¥c **\"Your Authtoken\"**, copy cÃ¡i token cá»§a báº¡n.\n3.  DÃ¡n token nÃ y vÃ o **Cell 2** á»Ÿ biáº¿n `NGROK_AUTH_TOKEN`.\n\n### 3. Cáº­p nháº­t Cáº¥u hÃ¬nh Model\n\nSau khi thÃªm Kaggle Dataset (á»Ÿ BÆ°á»›c 1), báº¡n sáº½ tháº¥y Ä‘Æ°á»ng dáº«n cá»§a nÃ³ (vÃ­ dá»¥: `/kaggle/input/my-models`). \n\nHÃ£y cáº­p nháº­t **Cell 2** á»Ÿ biáº¿n `MODEL_CONFIGS`:\n\n* **`ckpt_path`**: Sá»­a láº¡i Ä‘Æ°á»ng dáº«n cho Ä‘Ãºng vá»›i file model cá»§a báº¡n (vÃ­ dá»¥: `/kaggle/input/my-models/restaurant_best_model.ckpt`).\n* **`aspect_columns`**: Báº¡n pháº£i **dÃ¡n ÄÃšNG vÃ  Äá»¦** danh sÃ¡ch cÃ¡c cá»™t aspect cá»§a báº¡n vÃ o Ä‘Ã¢y. TÃ´i Ä‘Ã£ Ä‘á»ƒ cÃ¡c giÃ¡ trá»‹ vÃ­ dá»¥.","metadata":{}},{"id":"f45a5af3-c505-4f24-b423-031cd5c973ce","cell_type":"code","source":"# === Cell 1: CÃ i Ä‘áº·t thÆ° viá»‡n ===\n# ThÃªm 'sentencepiece' cho model LLM tÃ³m táº¯t (ViT5)\n\nprint(\"Äang cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n chÃ­nh...\")\n!pip install flask pyngrok pytorch-lightning transformers pandas sentencepiece -q\n\nprint(\"--- Sá»¬A Lá»–I: CÃ i Ä‘áº·t láº¡i Protobuf (Náº¿u cáº§n) ---\")\n!pip uninstall -y protobuf\n!pip install protobuf==3.20.3 -q\n\nprint(\"ÄÃ£ cÃ i Ä‘áº·t xong.\")\nprint(\"LÆ¯U Ã: Vui lÃ²ng Bá» QUA cÃ¡c lá»—i 'Unable to register' (náº¿u cÃ³).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:40:16.034988Z","iopub.execute_input":"2025-11-14T14:40:16.035176Z","iopub.status.idle":"2025-11-14T14:41:35.618328Z","shell.execute_reply.started":"2025-11-14T14:40:16.035159Z","shell.execute_reply":"2025-11-14T14:41:35.617527Z"}},"outputs":[{"name":"stdout","text":"Äang cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n chÃ­nh...\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m--- Sá»¬A Lá»–I: CÃ i Ä‘áº·t láº¡i Protobuf (Náº¿u cáº§n) ---\nFound existing installation: protobuf 6.33.0\nUninstalling protobuf-6.33.0:\n  Successfully uninstalled protobuf-6.33.0\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mÄÃ£ cÃ i Ä‘áº·t xong.\nLÆ¯U Ã: Vui lÃ²ng Bá» QUA cÃ¡c lá»—i 'Unable to register' (náº¿u cÃ³).\n","output_type":"stream"}],"execution_count":1},{"id":"6d67141a-cb11-4ebb-b3c4-05e5473a7606","cell_type":"markdown","source":"## ğŸ›‘ QUAN TRá»ŒNG: KHá»I Äá»˜NG Láº I KERNEL\n\nSau khi Cell 1 cháº¡y xong, hÃ£y **KHá»I Äá»˜NG Láº I KERNEL** Ä‘á»ƒ Ã¡p dá»¥ng báº£n `protobuf` Ä‘Ã£ sá»­a.\n\nTrÃªn thanh menu, chá»n **Run -> Restart Session**.\n\nSau Ä‘Ã³, hÃ£y **cháº¡y láº¡i Cell 1** má»™t láº§n ná»¯a (Ä‘á»ƒ load thÆ° viá»‡n), rá»“i **cháº¡y tiáº¿p tá»« Cell 2**.","metadata":{}},{"id":"97f88dbf-4e1c-4df4-8475-af181188c0ed","cell_type":"code","source":"# === Cell 2: Cáº¥u hÃ¬nh (Sá»¬A Lá»–I ÄÆ¯á»œNG DáºªN CKPT) ===\nimport os\nimport json\n\n# 1. DÃ¡n NGROK Auth Token cá»§a báº¡n vÃ o Ä‘Ã¢y\nNGROK_AUTH_TOKEN = \"33xssoeiIZT6i4jrZb0ZAOECVz6_7garq9w8GQ8kri2Ni92J\"  # <--- THAY THáº¾!\n\n# 2. Cáº¥u hÃ¬nh Model Checkpoints vÃ  Aspects\n#    Sá»¬A Láº I 'ckpt_path' cho Ä‘Ãºng vá»›i model cá»§a báº¡n\nMODEL_CONFIGS = {\n    \"restaurant\": {\n        # !!! KIá»‚M TRA Ká»¸ DÃ’NG NÃ€Y !!!\n        \"ckpt_path\": \"/kaggle/input/absa-model/absa_restaurant_best_model.ckpt\",  # <--- PHáº¢I LÃ€ FILE Cá»¦A RESTAURANT (12 aspect)\n        \"aspect_columns\": [\n            'AMBIENCE#GENERAL', 'DRINKS#PRICES', 'DRINKS#QUALITY', 'DRINKS#STYLE&OPTIONS', \n            'FOOD#PRICES', 'FOOD#QUALITY', 'FOOD#STYLE&OPTIONS', 'LOCATION#GENERAL', \n            'RESTAURANT#GENERAL', 'RESTAURANT#MISCELLANEOUS', 'RESTAURANT#PRICES', \n            'SERVICE#GENERAL'\n        ]\n    },\n    \"hotel\": {\n        # !!! KIá»‚M TRA Ká»¸ DÃ’NG NÃ€Y !!!\n        \"ckpt_path\": \"/kaggle/input/absa-model/absa_hotel_best_model.ckpt\",  # <--- PHáº¢I LÃ€ FILE Cá»¦A HOTEL (34 aspect)\n        \"aspect_columns\": [\n            'FACILITIES#CLEANLINESS', 'FACILITIES#COMFORT', 'FACILITIES#DESIGN&FEATURES', \n            'FACILITIES#GENERAL', 'FACILITIES#MISCELLANEOUS', 'FACILITIES#PRICES', \n            'FACILITIES#QUALITY', 'FOOD&DRINKS#MISCELLANEOUS', 'FOOD&DRINKS#PRICES', \n            'FOOD&DRINKS#QUALITY', 'FOOD&DRINKS#STYLE&OPTIONS', 'HOTEL#CLEANLINESS', \n            'HOTEL#COMFORT', 'HOTEL#DESIGN&FEATURES', 'HOTEL#GENERAL', 'HOTEL#MISCELLANEOUS', \n            'HOTEL#PRICES', 'HOTEL#QUALITY', 'LOCATION#GENERAL', 'ROOMS#CLEANLINESS', 'ROOMS#COMFORT',\n            'ROOMS#DESIGN&FEATURES', 'ROOMS#GENERAL', 'ROOMS#MISCELLANEOUS', 'ROOMS#PRICES', \n            'ROOMS#QUALITY', 'ROOM_AMENITIES#CLEANLINESS', 'ROOM_AMENITIES#COMFORT', 'ROOM_AMENITIES#DESIGN&FEATURES', \n            'ROOM_AMENITIES#GENERAL', 'ROOM_AMENITIES#MISCELLANEOUS', 'ROOM_AMENITIES#PRICES', \n            'ROOM_AMENITIES#QUALITY', 'SERVICE#GENERAL'\n        ]\n    }\n}\n\n# 3. Cáº¥u hÃ¬nh chung\n# Äá»“ng bá»™ tokenizer vá»›i encoder (cáº£ 2 Ä‘á»u lÃ  'large')\nTOKENIZER_NAME = \"VietAI/vit5-large\"\nLLM_SUMMARIZER_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\n# Táº¯t cáº£nh bÃ¡o cá»§a Tokenizers\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nprint(f\"Cáº¥u hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c táº£i.\")\nprint(f\"Sá»¬ Dá»¤NG TOKENIZER: {TOKENIZER_NAME}\") \nprint(f\"Restaurant aspects: {len(MODEL_CONFIGS['restaurant']['aspect_columns'])}\")\nprint(f\"Hotel aspects: {len(MODEL_CONFIGS['hotel']['aspect_columns'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.619527Z","iopub.execute_input":"2025-11-14T14:41:35.619730Z","iopub.status.idle":"2025-11-14T14:41:35.626938Z","shell.execute_reply.started":"2025-11-14T14:41:35.619707Z","shell.execute_reply":"2025-11-14T14:41:35.626402Z"}},"outputs":[{"name":"stdout","text":"Cáº¥u hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c táº£i.\nSá»¬ Dá»¤NG TOKENIZER: VietAI/vit5-large\nRestaurant aspects: 12\nHotel aspects: 34\n","output_type":"stream"}],"execution_count":2},{"id":"a5aac339-d57c-407b-ae2a-d66d1b6fcca8","cell_type":"code","source":"# === Cell 3: Ghi file Äá»‹nh nghÄ©a Model (model_definition.py) ===\n# Cell nÃ y khÃ´ng thay Ä‘á»•i, chá»©a kiáº¿n trÃºc model cá»§a báº¡n.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.628533Z","iopub.execute_input":"2025-11-14T14:41:35.628721Z","iopub.status.idle":"2025-11-14T14:41:35.646389Z","shell.execute_reply.started":"2025-11-14T14:41:35.628706Z","shell.execute_reply":"2025-11-14T14:41:35.645734Z"}},"outputs":[],"execution_count":3},{"id":"623b95f2-38f5-4b7e-a035-422391f5beeb","cell_type":"code","source":"!mkdir -p templates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.646947Z","iopub.execute_input":"2025-11-14T14:41:35.647136Z","iopub.status.idle":"2025-11-14T14:41:35.775179Z","shell.execute_reply.started":"2025-11-14T14:41:35.647117Z","shell.execute_reply":"2025-11-14T14:41:35.774349Z"}},"outputs":[],"execution_count":4},{"id":"c0b43174-2c4a-401d-820a-6a0b5e62d415","cell_type":"code","source":"%%writefile model_definition.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom transformers import T5EncoderModel\nimport math\n\n# --- Class Loss (Cáº§n Ä‘á»ƒ táº£i model) ---\nclass AsymmetricLossOptimized(nn.Module):\n    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n        super().__init__()\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n        self.clip = clip\n        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n        self.eps = eps\n        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n\n    def forward(self, x, y):\n        self.targets = y\n        self.anti_targets = 1 - y\n        self.xs_pos = torch.sigmoid(x)\n        self.xs_neg = 1.0 - self.xs_pos\n        if self.clip is not None and self.clip > 0:\n            self.xs_neg.add_(self.clip).clamp_(max=1)\n        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n        if self.gamma_neg > 0 or self.gamma_pos > 0:\n            if self.disable_torch_grad_focal_loss:\n                torch.set_grad_enabled(False)\n            self.xs_pos = self.xs_pos * self.targets\n            self.xs_neg = self.xs_neg * self.anti_targets\n            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n            if self.disable_torch_grad_focal_loss:\n                torch.set_grad_enabled(True)\n            self.loss *= self.asymmetric_w\n        return -self.loss.sum() / x.size(0)\n\n# --- CÃ¡c Sub-module cá»§a Kiáº¿n trÃºc Model ---\n\nclass NeighborhoodAttention(nn.Module):\n    def __init__(self, hidden_size: int, num_heads: int = 8, window_size: int = 7):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.head_dim = hidden_size // num_heads\n        self.qkv = nn.Linear(hidden_size, hidden_size * 3, bias=False)\n        self.proj = nn.Linear(hidden_size, hidden_size)\n        \n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n        B, L, C = x.shape\n        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, self.head_dim)\n        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n        attn_output = []\n        for i in range(L):\n            start = max(0, i - self.window_size // 2)\n            end = min(L, i + self.window_size // 2 + 1)\n            q_i = q[:, :, i:i+1, :]\n            k_window = k[:, :, start:end, :]\n            v_window = v[:, :, start:end, :]\n            scores = torch.matmul(q_i, k_window.transpose(-2, -1)) / math.sqrt(self.head_dim)\n            attn_weights = F.softmax(scores, dim=-1)\n            out_i = torch.matmul(attn_weights, v_window)\n            attn_output.append(out_i)\n        attn_output = torch.cat(attn_output, dim=2)\n        attn_output = attn_output.transpose(1, 2).reshape(B, L, C)\n        return self.proj(attn_output)\n\nclass StateConvBlock(nn.Module):\n    def __init__(self, hidden_size: int):\n        super().__init__()\n        inner_dim = hidden_size * 2\n        self.in_proj = nn.Linear(hidden_size, inner_dim * 2, bias=False)\n        self.conv1d = nn.Conv1d(inner_dim, inner_dim, kernel_size=3, padding=1, groups=inner_dim)\n        self.out_proj = nn.Linear(inner_dim, hidden_size, bias=False)\n        self.norm = nn.LayerNorm(hidden_size)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x_and_res = self.in_proj(x)\n        x_inner, res = x_and_res.split(x_and_res.shape[-1] // 2, dim=-1)\n        x_inner = self.conv1d(x_inner.transpose(1, 2)).transpose(1, 2)\n        x_inner = F.silu(x_inner)\n        y = x_inner * F.silu(res)\n        output = self.out_proj(y)\n        return x + self.norm(output)\n\nclass PositionBiasedAttention(nn.Module):\n    def __init__(self, hidden_size: int, num_aspects: int):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_aspects = num_aspects\n        self.attn = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.aspect_queries = nn.Parameter(torch.randn(num_aspects, hidden_size))\n    def _create_position_bias(self, seq_len: int, device: torch.device) -> torch.Tensor:\n        bias = torch.zeros(seq_len, seq_len, device=device)\n        for i in range(seq_len):\n            for j in range(seq_len):\n                distance = abs(i - j)\n                if distance > 5:\n                    bias[i, j] = -10.0\n        return bias\n    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        B, L, D = x.shape\n        pos_bias = self._create_position_bias(L, x.device)\n        attn_out, _ = self.attn(x, x, x, attn_mask=pos_bias)\n        aspect_features = []\n        for b in range(B):\n            seq_len = int(attention_mask[b].sum().item())\n            valid_keys = attn_out[b, :seq_len].unsqueeze(0)\n            queries = self.aspect_queries.unsqueeze(0)\n            attn_score = torch.matmul(queries, valid_keys.transpose(-2, -1)) / math.sqrt(D)\n            attn_weight = F.softmax(attn_score, dim=-1)\n            pooled = torch.matmul(attn_weight, valid_keys)\n            aspect_features.append(pooled.squeeze(0))\n        return torch.stack(aspect_features)\n\nclass AdvancedCNN(nn.Module):\n    def __init__(self, hidden_size: int, num_filters: int = 128):\n        super().__init__()\n        self.conv_layers = nn.ModuleList([\n            nn.Conv1d(hidden_size, num_filters, kernel_size=3, dilation=1, padding=1),\n            nn.Conv1d(hidden_size, num_filters, kernel_size=3, dilation=2, padding=2),\n            nn.Conv1d(hidden_size, num_filters, kernel_size=3, dilation=4, padding=4),\n        ])\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.fusion = nn.Linear(num_filters * len(self.conv_layers), hidden_size)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.transpose(1, 2)\n        conv_outputs = []\n        for conv in self.conv_layers:\n            conv_out = F.relu(conv(x))\n            pooled = self.pool(conv_out).squeeze(-1)\n            conv_outputs.append(pooled)\n        fused = torch.cat(conv_outputs, dim=-1)\n        return self.fusion(fused)\n\n# --- Class Model ChÃ­nh ---\nclass AdvancedABSAModel(pl.LightningModule):\n    def __init__(self, \n                 num_aspects=12, \n                 learning_rate=2e-5, \n                 encoder_name='VietAI/vit5-large',\n                 use_neighborhood_attn=True,\n                 use_state_conv=True,\n                 use_position_attn=True,\n                 use_advanced_cnn=True):\n        super().__init__()\n        self.save_hyperparameters() \n        \n        self.num_aspects = num_aspects\n        self.encoder = T5EncoderModel.from_pretrained(encoder_name)\n        hidden_size = self.encoder.config.d_model\n\n        self.use_neighborhood_attn = use_neighborhood_attn\n        self.use_state_conv = use_state_conv\n        self.use_position_attn = use_position_attn\n        self.use_advanced_cnn = use_advanced_cnn\n\n        if self.use_neighborhood_attn:\n            self.neighborhood_attn = NeighborhoodAttention(hidden_size, window_size=7)\n        if self.use_state_conv:\n            self.state_conv_blocks = nn.ModuleList([StateConvBlock(hidden_size) for _ in range(2)]) # Máº·c Ä‘á»‹nh lÃ  2 khá»‘i nhÆ° trong notebook\n        if self.use_position_attn:\n            self.position_biased_attn = PositionBiasedAttention(hidden_size, num_aspects)\n        if self.use_advanced_cnn:\n            self.advanced_cnn = AdvancedCNN(hidden_size)\n        \n        if self.use_position_attn and self.use_advanced_cnn:\n             self.fusion = nn.Linear(hidden_size * 2, hidden_size)\n        \n        self.classifiers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_size, hidden_size // 2),\n                nn.GELU(),\n                nn.Dropout(0.1),\n                nn.Linear(hidden_size // 2, 4)\n            ) for _ in range(num_aspects)\n        ])\n        \n        self.criterion = AsymmetricLossOptimized()\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        x = encoder_output.last_hidden_state\n    \n        if self.use_neighborhood_attn:\n            x_block = self.neighborhood_attn(x, attention_mask)\n        else:\n            x_block = x\n    \n        if self.use_state_conv:\n            for block in self.state_conv_blocks:\n                x_block = block(x_block)\n    \n        if self.use_position_attn:\n            aspect_features = self.position_biased_attn(x_block, attention_mask)\n        else:\n            aspect_features = x_block.mean(dim=1, keepdim=True).expand(-1, self.num_aspects, -1)\n    \n        if self.use_advanced_cnn:\n            cnn_features = self.advanced_cnn(x_block)\n        else:\n            cnn_features = x_block.mean(dim=1)\n    \n        cnn_features = cnn_features.unsqueeze(1).expand(-1, self.num_aspects, -1)\n        \n        if self.use_position_attn and self.use_advanced_cnn:\n            fused_features = torch.cat([aspect_features, cnn_features], dim=-1)\n            fused_features = self.fusion(fused_features)\n        elif self.use_position_attn:\n            fused_features = aspect_features\n        elif self.use_advanced_cnn:\n            fused_features = cnn_features\n        else:\n            fused_features = x_block.mean(dim=1, keepdim=True).expand(-1, self.num_aspects, -1)\n            \n        logits = []\n        for i, classifier in enumerate(self.classifiers):\n            aspect_logits = classifier(fused_features[:, i, :])\n            logits.append(aspect_logits)\n        logits = torch.stack(logits, dim=1)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.776406Z","iopub.execute_input":"2025-11-14T14:41:35.776983Z","iopub.status.idle":"2025-11-14T14:41:35.787121Z","shell.execute_reply.started":"2025-11-14T14:41:35.776956Z","shell.execute_reply":"2025-11-14T14:41:35.786596Z"}},"outputs":[{"name":"stdout","text":"Writing model_definition.py\n","output_type":"stream"}],"execution_count":5},{"id":"22b7b6cc-2432-479e-8e1e-62fb2ab83645","cell_type":"code","source":"# === Cell 4: Ghi file Logic Xá»­ lÃ½ (app_logic.py) ===\n# NÃ‚NG Cáº¤P: ThÃªm logic táº£i model LLM tÃ³m táº¯t","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.787862Z","iopub.execute_input":"2025-11-14T14:41:35.788322Z","iopub.status.idle":"2025-11-14T14:41:35.805635Z","shell.execute_reply.started":"2025-11-14T14:41:35.788294Z","shell.execute_reply":"2025-11-14T14:41:35.805057Z"}},"outputs":[],"execution_count":6},{"id":"5e1edaf3-bf34-4dbe-be18-1e29b2c52ff5","cell_type":"code","source":"%%writefile app_logic.py\nimport torch\nfrom transformers import AutoTokenizer, pipeline\nfrom model_definition import AdvancedABSAModel\nimport time\n\n# Biáº¿n global Ä‘á»ƒ cache model (giá»¯ model Ä‘Ã£ táº£i trong RAM)\nloaded_models_cache = {}\nloaded_summarizer_cache = None\n\nIDX_TO_SENTIMENT = {0: 'None', 1: 'positive', 2: 'negative', 3: 'neutral'}\nMAX_LENGTH = 128 # Giá»‘ng nhÆ° khi train\n\ndef get_model_components(model_type: str, model_configs: dict, tokenizer_name: str, device: torch.device):\n    \"\"\"Táº£i model ABSA vÃ  tokenizer. TÃ¡i sá»­ dá»¥ng náº¿u Ä‘Ã£ táº£i.\"\"\"\n    if model_type in loaded_models_cache:\n        print(f\"Using cached model for: {model_type}\")\n        return loaded_models_cache[model_type]\n    \n    print(f\"Cache miss. Loading model for: {model_type}...\")\n    start_time = time.time()\n    \n    try:\n        config = model_configs[model_type]\n        ckpt_path = config[\"ckpt_path\"]\n        aspect_columns = config[\"aspect_columns\"]\n    except KeyError:\n        raise ValueError(f\"Model type '{model_type}' khÃ´ng cÃ³ trong cáº¥u hÃ¬nh.\")\n\n    # 1. Táº£i Tokenizer (cache chung)\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    \n    # 2. Táº£i Model ABSA tá»« Checkpoint\n    num_aspects = len(aspect_columns)\n    try:\n        model = AdvancedABSAModel.load_from_checkpoint(\n            checkpoint_path=ckpt_path,\n            map_location=device,\n            num_aspects=num_aspects\n        )\n    except FileNotFoundError:\n        print(f\"Lá»–I: KhÃ´ng tÃ¬m tháº¥y file checkpoint táº¡i: {ckpt_path}\")\n        raise\n    except Exception as e:\n        print(f\"Lá»–I khi táº£i checkpoint: {e}\")\n        raise\n        \n    model.eval()\n    model.to(device)\n    \n    components = (model, tokenizer, aspect_columns)\n    loaded_models_cache[model_type] = components\n    \n    end_time = time.time()\n    print(f\"Model for '{model_type}' loaded in {end_time - start_time:.2f} seconds.\")\n    return components\n\ndef run_prediction(review_text: str, model: AdvancedABSAModel, tokenizer, aspect_columns: list, device: torch.device) -> dict:\n    \"\"\"Cháº¡y dá»± Ä‘oÃ¡n ABSA vÃ  tráº£ vá» dict káº¿t quáº£.\"\"\"\n    \n    encoding = tokenizer(\n        review_text,\n        truncation=True,\n        padding='max_length',\n        max_length=MAX_LENGTH,\n        return_tensors='pt'\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        logits = model(input_ids, attention_mask)\n        \n    preds = torch.argmax(logits, dim=-1).squeeze(0)\n    preds_cpu = preds.cpu().numpy()\n    \n    results = {}\n    for i, aspect_name in enumerate(aspect_columns):\n        sentiment_idx = preds_cpu[i]\n        sentiment_name = IDX_TO_SENTIMENT[sentiment_idx]\n        \n        if sentiment_name != 'None':\n            friendly_name = aspect_name.replace('#', ' (').replace('_', ' ') + ')'\n            if 'GENERAL' in friendly_name:\n                friendly_name = friendly_name.replace(' (GENERAL)', '')\n            results[friendly_name] = sentiment_name\n            \n    return results\n\n# *** HÃ€M ÄÃƒ Sá»¬A Lá»–I (1) ***\ndef get_summarizer(model_name: str, device: torch.device):\n    \"\"\"Táº£i model LLM (dÆ°á»›i dáº¡ng text-generation).\"\"\"\n    global loaded_summarizer_cache\n    if loaded_summarizer_cache:\n        print(\"Using cached generator (LLM).\")\n        return loaded_summarizer_cache\n    \n    print(f\"Loading TEXT-GENERATION pipeline for: {model_name}...\")\n    start_time = time.time()\n    \n    # Sá»¬A Lá»–I: DÃ¹ng 'text-generation' cho model Qwen/Llama\n    summarizer = pipeline(\n        \"text-generation\", # <--- THAY Äá»”I QUAN TRá»ŒNG\n        model=model_name,\n        device=0 if \"cuda\" in device.type else -1,\n        # ThÃªm trust_remote_code cho cÃ¡c model Qwen (báº¯t buá»™c)\n        trust_remote_code=True \n    )\n    \n    loaded_summarizer_cache = summarizer\n    end_time = time.time()\n    print(f\"Text-generation pipeline loaded in {end_time - start_time:.2f} seconds.\")\n    return summarizer\n\n# *** HÃ€M ÄÃƒ Sá»¬A Lá»–I (2) ***\ndef generate_natural_response(structured_results: dict, original_review: str, summarizer) -> str:\n    \"\"\"Táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn báº±ng Qwen Chat Template.\"\"\"\n    if not structured_results:\n        return \"Cáº£m Æ¡n báº¡n Ä‘Ã£ chia sáº», nhÆ°ng tÃ´i khÃ´ng tÃ¬m tháº¥y khÃ­a cáº¡nh nÃ o cá»¥ thá»ƒ trong review nÃ y.\"\n\n    # 1. Chuyá»ƒn dict káº¿t quáº£ thÃ nh chuá»—i\n    analysis_parts = []\n    for aspect, sentiment in structured_results.items():\n        if sentiment == 'positive': sentiment_vn = 'tÃ­ch cá»±c'\n        elif sentiment == 'negative': sentiment_vn = 'tiÃªu cá»±c'\n        else: sentiment_vn = 'trung tÃ­nh'\n        analysis_parts.append(f\"{aspect}: {sentiment_vn}\")\n    analysis_str = \", \".join(analysis_parts)\n    \n    # 2. *** PROMPT Má»šI (Dáº¡y model theo khuÃ´n máº«u cá»§a báº¡n) ***\n    # ÄÃ¢y lÃ  \"few-shot prompting\" Ä‘á»ƒ Ã©p model tuÃ¢n thá»§\n    system_prompt = (\n        \"Báº¡n lÃ  má»™t trá»£ lÃ½ AI, chá»‰ tráº£ lá»i báº±ng tiáº¿ng Viá»‡t. \"\n        \"Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  chuyá»ƒn danh sÃ¡ch phÃ¢n tÃ­ch (Input) thÃ nh má»™t cÃ¢u tÃ³m táº¯t (Output) *duy nháº¥t* theo khuÃ´n máº«u: 'NgÆ°á»i dÃ¹ng cáº£m tháº¥y [cáº£m xÃºc] vá» [khÃ­a cáº¡nh]'. \"\n        \"Khi cÃ³ nhiá»u khÃ­a cáº¡nh, hÃ£y dÃ¹ng tá»« ná»‘i (nhÆ° 'vÃ ', 'nhÆ°ng', 'trong khi') Ä‘á»ƒ táº¡o thÃ nh má»™t cÃ¢u hoÃ n chá»‰nh.\\n\\n\"\n        \"VÃ­ dá»¥ 1:\\n\"\n        \"Input: \\\"FOOD (QUALITY): tÃ­ch cá»±c\\\"\\n\"\n        \"Output: NgÆ°á»i dÃ¹ng cáº£m tháº¥y tÃ­ch cá»±c vá» cháº¥t lÆ°á»£ng Ä‘á»“ Äƒn.\\n\\n\"\n        \"VÃ­ dá»¥ 2:\\n\"\n        \"Input: \\\"FOOD (QUALITY): tÃ­ch cá»±c, SERVICE (GENERAL): tiÃªu cá»±c\\\"\\n\"\n        \"Output: NgÆ°á»i dÃ¹ng cáº£m tháº¥y tÃ­ch cá»±c vá» cháº¥t lÆ°á»£ng Ä‘á»“ Äƒn, nhÆ°ng tiÃªu cá»±c vá» dá»‹ch vá»¥.\\n\\n\"\n        \"VÃ­ dá»¥ 3:\\n\"\n        \"Input: \\\"SERVICE (GENERAL): tÃ­ch cá»±c, DRINKS (QUALITY): tÃ­ch cá»±c, FOOD (PRICES): trung tÃ­nh\\\"\\n\"\n        \"Output: NgÆ°á»i dÃ¹ng cáº£m tháº¥y tÃ­ch cá»±c vá» dá»‹ch vá»¥ vÃ  cháº¥t lÆ°á»£ng Ä‘á»“ uá»‘ng, trong khi giÃ¡ Ä‘á»“ Äƒn á»Ÿ má»©c trung bÃ¬nh.\"\n    )\n\n    prompt = (\n        f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n        \"<|im_start|>user\\n\"\n        f\"Input: \\\"{analysis_str}\\\"<|im_end|>\\n\"\n        \"<|im_start|>assistant\\nOutput: \" # ThÃªm \"Output: \" Ä‘á»ƒ kÃ­ch hoáº¡t (trigger)\n    )\n    \n    # 3. Cháº¡y LLM\n    try:\n        outputs = summarizer(\n            prompt,\n            max_new_tokens=100, # Giá»›i háº¡n 100 token\n            min_length=5,\n            do_sample=False,\n            num_beams=4,\n            # YÃªu cáº§u pipeline dá»«ng khi gáº·p token káº¿t thÃºc cÃ¢u\n            eos_token_id=summarizer.tokenizer.eos_token_id, \n            # Chá»‰ tráº£ vá» pháº§n text má»›i, khÃ´ng láº·p láº¡i prompt\n            return_full_text=False \n        )\n        # Láº¥y káº¿t quáº£, xÃ³a \"Output: \" (náº¿u lá»¡ cÃ³) vÃ  xÃ³a khoáº£ng tráº¯ng\n        generated_text = outputs[0]['generated_text'].strip()\n        if generated_text.startswith(\"Output:\"):\n            generated_text = generated_text[len(\"Output:\"):].strip()\n        return generated_text\n        \n    except Exception as e:\n        print(f\"Lá»—i khi tÃ³m táº¯t: {e}\")\n        return \"Xin lá»—i, tÃ´i gáº·p lá»—i khi táº¡o tÃ³m táº¯t.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.806319Z","iopub.execute_input":"2025-11-14T14:41:35.806485Z","iopub.status.idle":"2025-11-14T14:41:35.823497Z","shell.execute_reply.started":"2025-11-14T14:41:35.806471Z","shell.execute_reply":"2025-11-14T14:41:35.822781Z"}},"outputs":[{"name":"stdout","text":"Writing app_logic.py\n","output_type":"stream"}],"execution_count":7},{"id":"794ce671-0439-49e3-bbc3-6884f7482e27","cell_type":"code","source":"# === Cell 5: Ghi file HTML (templates/index.html) ===\n# NÃ‚NG Cáº¤P: Táº¡o thÆ° má»¥c 'templates' vÃ  file HTML cho frontend","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.824361Z","iopub.execute_input":"2025-11-14T14:41:35.824580Z","iopub.status.idle":"2025-11-14T14:41:35.841510Z","shell.execute_reply.started":"2025-11-14T14:41:35.824555Z","shell.execute_reply":"2025-11-14T14:41:35.840847Z"}},"outputs":[],"execution_count":8},{"id":"8291d76b-8ce1-4dca-889d-de1203e078a2","cell_type":"code","source":"!mkdir -p templates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.843679Z","iopub.execute_input":"2025-11-14T14:41:35.843917Z","iopub.status.idle":"2025-11-14T14:41:35.969647Z","shell.execute_reply.started":"2025-11-14T14:41:35.843900Z","shell.execute_reply":"2025-11-14T14:41:35.968620Z"}},"outputs":[],"execution_count":9},{"id":"30bbaf56-9925-4986-a508-ad6b3eba25aa","cell_type":"code","source":"%%writefile templates/index.html\n<!DOCTYPE html>\n<html lang=\"vi\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Bot PhÃ¢n TÃ­ch Review</title>\n    <style>\n        body { font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif; \n               background-color: #f4f7f6; margin: 0; padding: 20px; display: flex; justify-content: center; }\n        .container { max-width: 700px; width: 100%; background-color: #ffffff; \n                     border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); padding: 30px; }\n        h1 { color: #333; text-align: center; margin-top: 0; }\n        form { display: flex; flex-direction: column; gap: 15px; }\n        textarea, select { width: 100%; padding: 12px; border-radius: 8px; border: 1px solid #ddd; \n                           font-size: 16px; box-sizing: border-box; }\n        button { background-color: #007aff; color: white; padding: 15px; border: none; \n                 border-radius: 8px; font-size: 16px; font-weight: 600; cursor: pointer; transition: background-color 0.2s; }\n        button:hover { background-color: #005bb5; }\n        button:disabled { background-color: #cddc39; cursor: not-allowed; }\n        .loader { text-align: center; font-weight: 600; color: #555; display: none; margin-top: 20px; }\n        .results { margin-top: 30px; border-top: 1px solid #eee; padding-top: 20px; }\n        .results h2 { margin-top: 0; color: #007aff; }\n        .results pre { background-color: #f0f0f0; padding: 15px; border-radius: 8px; \n                       white-space: pre-wrap; word-wrap: break-word; font-family: monospace; }\n        .summary { background-color: #e6f7ff; border: 1px solid #b3e0ff; padding: 15px; \n                   border-radius: 8px; font-size: 1.1em; color: #0056b3; line-height: 1.6; }\n        .review-echo { border-left: 4px solid #ccc; padding-left: 10px; font-style: italic; color: #555; }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>ğŸ¤– Bot PhÃ¢n TÃ­ch Review</h1>\n        <form id=\"review-form\" action=\"/predict\" method=\"POST\">\n            <label for=\"review\">Nháº­p review cá»§a báº¡n:</label>\n            <textarea id=\"review\" name=\"review\" rows=\"6\" required>{{ results.review if results }}</textarea>\n\n            <label for=\"model_type\">Chá»n loáº¡i hÃ¬nh:</label>\n            <select id=\"model_type\" name=\"model_type\" required>\n                <option value=\"restaurant\" {% if results and results.model_type == 'restaurant' %}selected{% endif %}>NhÃ  hÃ ng</option>\n                <option value=\"hotel\" {% if results and results.model_type == 'hotel' %}selected{% endif %}>KhÃ¡ch sáº¡n</option>\n            </select>\n\n            <button id=\"submit-btn\" type=\"submit\">PhÃ¢n tÃ­ch</button>\n        </form>\n        \n        <div id=\"loader\" class=\"loader\">\n            Äang phÃ¢n tÃ­ch... (Láº§n Ä‘áº§u cÃ³ thá»ƒ máº¥t 1-2 phÃºt Ä‘á»ƒ táº£i model) â³\n        </div>\n\n        {% if results %}\n        <div class=\"results\">\n            <h2>Káº¿t quáº£ PhÃ¢n tÃ­ch</h2>\n\n            <h3>TÃ³m táº¯t cá»§a Bot:</h3>\n            <p class=\"summary\">{{ results.natural_summary }}</p>\n\n            <h3>Review cá»§a báº¡n:</h3>\n            <p class=\"review-echo\">{{ results.review }}</p>\n\n            <h3>Chi tiáº¿t (Dáº¡ng JSON):</h3>\n            <pre>{{ results.structured_analysis | tojson(indent=4) }}</pre>\n        </div>\n        {% endif %}\n    </div>\n\n    <script>\n        // Hiá»ƒn thá»‹ loader khi form Ä‘Æ°á»£c submit\n        document.getElementById('review-form').addEventListener('submit', function() {\n            document.getElementById('submit-btn').disabled = true;\n            document.getElementById('submit-btn').innerText = 'Äang xá»­ lÃ½...';\n            document.getElementById('loader').style.display = 'block';\n        });\n    </script>\n</body>\n</html>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.971040Z","iopub.execute_input":"2025-11-14T14:41:35.971358Z","iopub.status.idle":"2025-11-14T14:41:35.978969Z","shell.execute_reply.started":"2025-11-14T14:41:35.971325Z","shell.execute_reply":"2025-11-14T14:41:35.978282Z"}},"outputs":[{"name":"stdout","text":"Writing templates/index.html\n","output_type":"stream"}],"execution_count":10},{"id":"3e924bf3-13d7-428e-bbf5-e5d3478fac7f","cell_type":"code","source":"# === Cell 6: Ghi file API ChÃ­nh (main_app.py) ===\n# NÃ‚NG Cáº¤P: \n# 1. Sá»­a lá»—i hardcode config (Ä‘á»c tá»« biáº¿n 'MODEL_CONFIGS' cá»§a notebook)\n# 2. ThÃªm route '/' Ä‘á»ƒ phá»¥c vá»¥ file HTML\n# 3. NÃ¢ng cáº¥p route '/predict' Ä‘á»ƒ nháº­n cáº£ JSON (API) vÃ  Form (HTML)\n# 4. Gá»i model LLM tÃ³m táº¯t vÃ  tráº£ vá» káº¿t quáº£","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.979874Z","iopub.execute_input":"2025-11-14T14:41:35.980199Z","iopub.status.idle":"2025-11-14T14:41:35.997817Z","shell.execute_reply.started":"2025-11-14T14:41:35.980169Z","shell.execute_reply":"2025-11-14T14:41:35.997015Z"}},"outputs":[],"execution_count":11},{"id":"fcbb08bc-0cbd-452f-8fb2-509821c99841","cell_type":"code","source":"# DÃ¹ng cÃ¡ch nÃ y Ä‘á»ƒ tiÃªm (inject) biáº¿n MODEL_CONFIGS tá»« Cell 2 vÃ o file .py\n# TrÃ¡nh bá»‹ lá»—i hardcode nhÆ° phiÃªn báº£n trÆ°á»›c\nfile_content = f\"\"\"\nimport torch\nfrom flask import Flask, request, jsonify, render_template\nimport app_logic\nimport threading\nimport json\n\n# --- Cáº¤U HÃŒNH ÄÆ¯á»¢C TIÃŠM Tá»ª NOTEBOOK ---\nMODEL_CONFIGS_FROM_NOTEBOOK = {json.dumps(MODEL_CONFIGS)}\nTOKENIZER_NAME_FROM_NOTEBOOK = \"{TOKENIZER_NAME}\"\nLLM_NAME_FROM_NOTEBOOK = \"{LLM_SUMMARIZER_NAME}\"\n# -----------------------------------\n\napp = Flask(__name__, template_folder='templates')\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Flask App running on device: {{DEVICE}}\")\n\n# Lock Ä‘á»ƒ trÃ¡nh 2 request cÃ¹ng lÃºc táº£i model\nmodel_load_lock = threading.Lock()\n\n@app.route('/')\ndef home():\n    # Phá»¥c vá»¥ file templates/index.html\n    return render_template('index.html')\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    try:\n        is_api_call = request.is_json\n        \n        if is_api_call:\n            data = request.json\n            review = data.get('review')\n            model_type = data.get('model_type') # \"restaurant\" or \"hotel\"\n        else:\n            # Láº¥y tá»« form HTML\n            data = request.form\n            review = data.get('review')\n            model_type = data.get('model_type')\n\n        if not review or not model_type:\n            error = {{\"error\": \"Missing 'review' or 'model_type'\"}}\n            return jsonify(error), 400 if is_api_call else render_template('index.html', results=error)\n\n        if model_type not in MODEL_CONFIGS_FROM_NOTEBOOK:\n            error = {{\"error\": f\"Invalid model_type. Choose from: {{list(MODEL_CONFIGS_FROM_NOTEBOOK.keys())}}\"}}\n            return jsonify(error), 400 if is_api_call else render_template('index.html', results=error)\n        \n        # --- Táº£i Model --- \n        # DÃ¹ng lock Ä‘á»ƒ Ä‘áº£m báº£o an toÃ n khi táº£i model (thread-safe)\n        with model_load_lock:\n            # 1. Táº£i model ABSA\n            model, tokenizer, aspect_cols = app_logic.get_model_components(\n                model_type, MODEL_CONFIGS_FROM_NOTEBOOK, TOKENIZER_NAME_FROM_NOTEBOOK, DEVICE\n            )\n            # 2. Táº£i model LLM TÃ³m táº¯t\n            summarizer = app_logic.get_summarizer(LLM_NAME_FROM_NOTEBOOK, DEVICE)\n        \n        # --- Cháº¡y Inference ---\n        # 1. Cháº¡y model ABSA\n        structured_results = app_logic.run_prediction(review, model, tokenizer, aspect_cols, DEVICE)\n        \n        # 2. Cháº¡y model LLM TÃ³m táº¯t\n        natural_summary = app_logic.generate_natural_response(structured_results, review, summarizer)\n        \n        # --- Tráº£ káº¿t quáº£ ---\n        final_results = {{\n            \"review\": review,\n            \"model_type\": model_type,\n            \"natural_summary\": natural_summary,\n            \"structured_analysis\": structured_results\n        }}\n        \n        if is_api_call:\n            # Náº¿u lÃ  API, tráº£ JSON\n            return jsonify(final_results)\n        else:\n            # Náº¿u lÃ  Form, render láº¡i trang HTML vá»›i káº¿t quáº£\n            return render_template('index.html', results=final_results)\n\n    except Exception as e:\n        print(f\"Error during prediction: {{e}}\")\n        error = {{\"error\": str(e)}}\n        return jsonify(error), 500 if is_api_call else render_template('index.html', results=error)\n\ndef run_server():\n    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n\"\"\"\n\n# Ghi ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c chÃ¨n biáº¿n vÃ o file\nwith open(\"main_app.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(file_content)\n\nprint(\"ÄÃ£ ghi file main_app.py vÃ  tiÃªm (inject) cáº¥u hÃ¬nh thÃ nh cÃ´ng.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:35.998807Z","iopub.execute_input":"2025-11-14T14:41:35.999480Z","iopub.status.idle":"2025-11-14T14:41:36.018270Z","shell.execute_reply.started":"2025-11-14T14:41:35.999461Z","shell.execute_reply":"2025-11-14T14:41:36.017634Z"}},"outputs":[{"name":"stdout","text":"ÄÃ£ ghi file main_app.py vÃ  tiÃªm (inject) cáº¥u hÃ¬nh thÃ nh cÃ´ng.\n","output_type":"stream"}],"execution_count":12},{"id":"fac035ba-f547-48a5-9fc7-8a1122a6c469","cell_type":"code","source":"# === Cell 7: Ghi file API ChÃ­nh (main_app.py) ===\n# Sá»¬A Lá»–I: Chá»‰ pre-load model Summarizer Ä‘á»ƒ tiáº¿t kiá»‡m VRAM\n# Sá»¬A Lá»–I Láº¦N 3: Sá»­a IndentationError trong khá»‘i 'except'\n\n# DÃ¹ng cÃ¡ch nÃ y Ä‘á»ƒ tiÃªm (inject) biáº¿n MODEL_CONFIGS tá»« Cell 2 vÃ o file .py\nfile_content = f\"\"\"\nimport torch\nfrom flask import Flask, request, jsonify, render_template\nimport app_logic\nimport threading\nimport json\nimport time\n\n# --- Cáº¤U HÃŒNH ÄÆ¯á»¢C TIÃŠM Tá»ª NOTEBOOK ---\nMODEL_CONFIGS_FROM_NOTEBOOK = {json.dumps(MODEL_CONFIGS)}\nTOKENIZER_NAME_FROM_NOTEBOOK = \"{TOKENIZER_NAME}\"\nLLM_NAME_FROM_NOTEBOOK = \"{LLM_SUMMARIZER_NAME}\"\n# ----------------------------------\n\napp = Flask(__name__, template_folder='templates')\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Flask App running on device: {{DEVICE}}\")\n\n# --- KHá»I Äá»˜NG VÃ€ Táº¢I TRÆ¯á»šC MODEL TÃ“M Táº®T ---\n# Chá»‰ táº£i LLM Summarizer (dÃ¹ng chung) Ä‘á»ƒ tiáº¿t kiá»‡m VRAM\ndef pre_load_summarizer():\n    print(\"=\"*30)\n    print(\"Báº®T Äáº¦U Táº¢I TRÆ¯á»šC MODEL SUMMARIZER (WARM-UP)...\")\n    \n    start_llm = time.time()\n    try:\n        app_logic.get_summarizer(LLM_NAME_FROM_NOTEBOOK, DEVICE)\n        # Sá»¬A Lá»–I F-STRING (thÃªm {{...}})\n        print(f\"-> ÄÃ£ táº£i xong LLM Summarizer (máº¥t {{time.time() - start_llm:.2f}}s)\")\n    except Exception as e:\n        print(f\"!!! Lá»—i khi táº£i LLM Summarizer: {{e}}\")\n\n    print(\"HOÃ€N Táº¤T Táº¢I TRÆ¯á»šC. CÃ¡c model ABSA sáº½ Ä‘Æ°á»£c táº£i khi cÃ³ request Ä‘áº§u tiÃªn.\")\n    print(\"Server Flask Ä‘ang khá»Ÿi Ä‘á»™ng...\")\n    print(\"=\"*30)\n\npre_load_summarizer()\n# ----------------------------------\n\n# Lock Ä‘á»ƒ trÃ¡nh 2 request cÃ¹ng lÃºc táº£i model\nmodel_load_lock = threading.Lock()\n\n@app.route('/')\ndef home():\n    # Phá»¥c vá»¥ file templates/index.html\n    return render_template('index.html')\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    try:\n        is_api_call = request.is_json\n        \n        if is_api_call:\n            data = request.json\n            review = data.get('review')\n            model_type = data.get('model_type') # \"restaurant\" or \"hotel\"\n        else:\n            # Láº¥y tá»« form HTML\n            data = request.form\n            review = data.get('review')\n            model_type = data.get('model_type')\n\n        if not review or not model_type:\n            error = {{\"error\": \"Missing 'review' or 'model_type'\"}}\n            return jsonify(error), 400 if is_api_call else render_template('index.html', results=error)\n\n        if model_type not in MODEL_CONFIGS_FROM_NOTEBOOK:\n            error = {{\"error\": f\"Invalid model_type. Choose from: {{list(MODEL_CONFIGS_FROM_NOTEBOOK.keys())}}\"}}\n            return jsonify(error), 400 if is_api_call else render_template('index.html', results=error)\n        \n        # --- Táº£i Model (Lazy-load) --- \n        # DÃ¹ng lock Ä‘á»ƒ Ä‘áº£m báº£o an toÃ n khi táº£i model (thread-safe)\n        with model_load_lock:\n            # 1. Táº£i model ABSA (Sáº½ táº£i náº¿u chÆ°a cÃ³, hoáº·c dÃ¹ng cache náº¿u Ä‘Ã£ táº£i)\n            model, tokenizer, aspect_cols = app_logic.get_model_components(\n                model_type, MODEL_CONFIGS_FROM_NOTEBOOK, TOKENIZER_NAME_FROM_NOTEBOOK, DEVICE\n            )\n            # 2. Láº¥y model LLM TÃ³m táº¯t (ÄÃ£ Ä‘Æ°á»£c pre-load, cháº¡y ráº¥t nhanh)\n            summarizer = app_logic.get_summarizer(LLM_NAME_FROM_NOTEBOOK, DEVICE)\n        \n        # --- Cháº¡y Inference ---\n        structured_results = app_logic.run_prediction(review, model, tokenizer, aspect_cols, DEVICE)\n        natural_summary = app_logic.generate_natural_response(structured_results, review, summarizer)\n        \n        # --- Tráº£ káº¿t quáº£ ---\n        final_results = {{\n            \"review\": review,\n            \"model_type\": model_type,\n            \"natural_summary\": natural_summary,\n            \"structured_analysis\": structured_results\n        }}\n        \n        if is_api_call:\n            return jsonify(final_results)\n        else:\n            return render_template('index.html', results=final_results)\n\n    except Exception as e:\n        # *** ÄÃƒ Sá»¬A Lá»–I THá»¤T Lá»€ á» 3 DÃ’NG DÆ¯á»šI (tá»« 12 vá» 8 dáº¥u cÃ¡ch) ***\n        print(f\"Error during prediction: {{e}}\")\n        error = {{\"error\": str(e)}}\n        return jsonify(error), 500 if is_api_call else render_template('index.html', results=error)\n\ndef run_server():\n    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n\"\"\"\n\n# Ghi ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c chÃ¨n biáº¿n vÃ o file\nwith open(\"main_app.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(file_content)\n\nprint(\"ÄÃ£ ghi file main_app.py (vá»›i logic pre-loading ÄÃƒ Sá»¬A Lá»–I) thÃ nh cÃ´ng.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:41:36.019002Z","iopub.execute_input":"2025-11-14T14:41:36.019264Z","iopub.status.idle":"2025-11-14T14:41:36.037698Z","shell.execute_reply.started":"2025-11-14T14:41:36.019241Z","shell.execute_reply":"2025-11-14T14:41:36.037085Z"}},"outputs":[{"name":"stdout","text":"ÄÃ£ ghi file main_app.py (vá»›i logic pre-loading ÄÃƒ Sá»¬A Lá»–I) thÃ nh cÃ´ng.\n","output_type":"stream"}],"execution_count":13},{"id":"583649c6-59a1-47ad-861c-841acdb3e403","cell_type":"code","source":"# === Cell 8: Khá»Ÿi cháº¡y API vÃ  Ngrok ===\n# Sá»¬A Lá»–I: ThÃªm ngrok.kill() vÃ  bá» time.sleep()\n\nfrom pyngrok import ngrok, conf\nimport threading\nimport time\n\n# Ngáº¯t má»i káº¿t ná»‘i Ngrok cÅ© (náº¿u cÃ³) trÆ°á»›c khi báº¯t Ä‘áº§u\nprint(\"Ngáº¯t cÃ¡c káº¿t ná»‘i Ngrok cÅ© (náº¿u cÃ³)...\")\nngrok.kill()\n\nprint(\"Äang thiáº¿t láº­p Ngrok Auth Token...\")\nconf.get_default().auth_token = NGROK_AUTH_TOKEN\n\n# --- QUAN TRá»ŒNG ---\n# Khi dÃ²ng 'import main_app' nÃ y cháº¡y, \n# nÃ³ sáº½ kÃ­ch hoáº¡t hÃ m pre_load_all_models() bÃªn trong file Ä‘Ã³.\n# Output sáº½ hiá»ƒn thá»‹ quÃ¡ trÃ¬nh táº£i model á»Ÿ Ä‘Ã¢y.\nprint(\"Äang import main_app (sáº½ kÃ­ch hoáº¡t pre-loading model)...\")\nimport main_app\n\nprint(\"Äang khá»Ÿi cháº¡y Flask server trong background...\")\nflask_thread = threading.Thread(target=main_app.run_server, daemon=True)\nflask_thread.start()\n\n# KhÃ´ng cáº§n time.sleep(3) ná»¯a, vÃ¬ chÃºng ta Ä‘Ã£ chá» model táº£i xong.\n\n# 2. Khá»Ÿi táº¡o Ngrok Tunnel\ntry:\n    print(\"Äang káº¿t ná»‘i Ngrok...\")\n    public_url = ngrok.connect(5000) # Káº¿t ná»‘i tá»›i port 5000\n    print(\"=\"*60)\n    print(f\"âœ… á»¨NG Dá»¤NG WEB Cá»¦A Báº N ÄÃƒ ONLINE! âœ…\")\n    print(f\"Truy cáº­p URL nÃ y trÃªn trÃ¬nh duyá»‡t: {public_url}\")\n    print(\"=\"*60)\n    print(\"Táº¥t cáº£ model Ä‘Ã£ Ä‘Æ°á»£c táº£i, API sáºµn sÃ ng nháº­n request ngay láº­p tá»©c.\")\nexcept Exception as e:\n    print(f\"Lá»—i khi khá»Ÿi táº¡o Ngrok: {e}\")\n    print(\"Vui lÃ²ng kiá»ƒm tra láº¡i NGROK_AUTH_TOKEN á»Ÿ Cell 2.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:42:08.027834Z","iopub.execute_input":"2025-11-14T14:42:08.028097Z","iopub.status.idle":"2025-11-14T14:42:35.078265Z","shell.execute_reply.started":"2025-11-14T14:42:08.028081Z","shell.execute_reply":"2025-11-14T14:42:35.077413Z"}},"outputs":[{"name":"stdout","text":"Ngáº¯t cÃ¡c káº¿t ná»‘i Ngrok cÅ© (náº¿u cÃ³)...\nÄang thiáº¿t láº­p Ngrok Auth Token...\nÄang import main_app (sáº½ kÃ­ch hoáº¡t pre-loading model)...\nFlask App running on device: cuda\n==============================\nBáº®T Äáº¦U Táº¢I TRÆ¯á»šC MODEL SUMMARIZER (WARM-UP)...\nLoading TEXT-GENERATION pipeline for: Qwen/Qwen2.5-1.5B-Instruct...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6ac5e45f2e0418e8399a76f09289253"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2afc6a0df464b54ad1a0ee4a9891149"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f24f50b430945beb803afdb26e159ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f077f7de76fa4d71bb492efdd1da5dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e7df07a289d40a7b0536d2d6f0fc457"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16857add512f43df9743a4b2f02a612a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"691c960898814cd58e280caddf051678"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Text-generation pipeline loaded in 17.84 seconds.\n-> ÄÃ£ táº£i xong LLM Summarizer (máº¥t 17.84s)\nHOÃ€N Táº¤T Táº¢I TRÆ¯á»šC. CÃ¡c model ABSA sáº½ Ä‘Æ°á»£c táº£i khi cÃ³ request Ä‘áº§u tiÃªn.\nServer Flask Ä‘ang khá»Ÿi Ä‘á»™ng...\n==============================\nÄang khá»Ÿi cháº¡y Flask server trong background...\nÄang káº¿t ná»‘i Ngrok...\n * Serving Flask app 'main_app'\n * Debug mode: off\n","output_type":"stream"},{"name":"stderr","text":"INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n * Running on all addresses (0.0.0.0)\n * Running on http://127.0.0.1:5000\n * Running on http://172.19.2.2:5000\nINFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"============================================================                                        \nâœ… á»¨NG Dá»¤NG WEB Cá»¦A Báº N ÄÃƒ ONLINE! âœ…\nTruy cáº­p URL nÃ y trÃªn trÃ¬nh duyá»‡t: NgrokTunnel: \"https://bilious-furfuraceously-franchesca.ngrok-free.dev\" -> \"http://localhost:5000\"\n============================================================\nTáº¥t cáº£ model Ä‘Ã£ Ä‘Æ°á»£c táº£i, API sáºµn sÃ ng nháº­n request ngay láº­p tá»©c.\n","output_type":"stream"},{"name":"stderr","text":"INFO:werkzeug:127.0.0.1 - - [14/Nov/2025 14:43:00] \"GET / HTTP/1.1\" 200 -\n","output_type":"stream"},{"name":"stdout","text":"Cache miss. Loading model for: restaurant...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/640 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b0766c5064d409882f70bfa72314fcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/820k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eda3d12a6f7347aba372fadf06f621ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39b5ba88ac4f404e9b389826a530eaba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a701820478ca4cdcbc8147a10be203cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47487b5abbf948a5b2c23f0febbc230b"}},"metadata":{}},{"name":"stdout","text":"Model for 'restaurant' loaded in 40.63 seconds.\nUsing cached generator (LLM).\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nINFO:werkzeug:127.0.0.1 - - [14/Nov/2025 14:46:09] \"POST /predict HTTP/1.1\" 200 -\n","output_type":"stream"},{"name":"stdout","text":"Using cached model for: restaurant\nUsing cached generator (LLM).\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nINFO:werkzeug:127.0.0.1 - - [14/Nov/2025 14:47:13] \"POST /predict HTTP/1.1\" 200 -\n","output_type":"stream"},{"name":"stdout","text":"Using cached model for: restaurant\nUsing cached generator (LLM).\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nINFO:werkzeug:127.0.0.1 - - [14/Nov/2025 14:47:26] \"POST /predict HTTP/1.1\" 200 -\n","output_type":"stream"},{"name":"stdout","text":"Using cached model for: restaurant\nUsing cached generator (LLM).\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nINFO:werkzeug:127.0.0.1 - - [14/Nov/2025 14:47:37] \"POST /predict HTTP/1.1\" 200 -\n","output_type":"stream"},{"name":"stdout","text":"Cache miss. Loading model for: hotel...\nModel for 'hotel' loaded in 28.92 seconds.\nUsing cached generator (LLM).\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nINFO:werkzeug:127.0.0.1 - - [14/Nov/2025 14:49:27] \"POST /predict HTTP/1.1\" 200 -\n","output_type":"stream"}],"execution_count":15},{"id":"b62f7c2f-51ae-4dce-b212-e75d76d60ed6","cell_type":"markdown","source":"## Don't Run Last Cell if you just start","metadata":{}},{"id":"ffebdd1a-d085-44cd-9f57-bf88bb4f6dbb","cell_type":"code","source":"# === Cell 9: Reset Server & XÃ³a Cache GPU ===\n# CHáº Y CELL NÃ€Y TRÆ¯á»šC KHI CHáº Y Láº I CELL 8\n\nimport sys\nimport torch\nfrom pyngrok import ngrok\n\nprint(\"Ngáº¯t káº¿t ná»‘i Ngrok cÅ©...\")\nngrok.kill()\n\n# 1. XÃ³a cache cá»§a Python (quan trá»ng nháº¥t)\n# Buá»™c Python pháº£i import láº¡i file .py khi cháº¡y Cell 8\ntry:\n    if 'main_app' in sys.modules:\n        del sys.modules['main_app']\n        print(\"-> ÄÃ£ xÃ³a module 'main_app' khá»i cache.\")\n        \n    if 'app_logic' in sys.modules:\n        del sys.modules['app_logic']\n        print(\"-> ÄÃ£ xÃ³a module 'app_logic' khá»i cache.\")\nexcept Exception as e:\n    print(f\"Lá»—i khi xÃ³a module cache: {e}\")\n\n# 2. Giáº£i phÃ³ng VRAM cá»§a GPU\nprint(\"Äang giáº£i phÃ³ng VRAM (GPU)...\")\ntorch.cuda.empty_cache()\n\nprint(\"\\nâœ… Reset hoÃ n táº¥t!\")\nprint(\"BÃ¢y giá» báº¡n cÃ³ thá»ƒ CHáº Y Láº I CELL 8 Ä‘á»ƒ táº£i model má»›i.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:42:05.193584Z","iopub.status.idle":"2025-11-14T14:42:05.193894Z","shell.execute_reply.started":"2025-11-14T14:42:05.193723Z","shell.execute_reply":"2025-11-14T14:42:05.193740Z"}},"outputs":[],"execution_count":null},{"id":"1a4f79c1-93b5-406f-af4c-cf1b0ce1fa0e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}